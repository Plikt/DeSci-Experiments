{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/desot1/Documents/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#setup \n",
    "import openai as openai #Extracting content metadata\n",
    "import fitz #pdf reading library\n",
    "import time #to ensure we don't call too often from openai\n",
    "from bs4 import BeautifulSoup #to extract XML info -> will be eliminated eventually\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import math\n",
    "import json\n",
    "\n",
    "# Library to import pre-trained model for sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Calculate similarities between sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# package for finding local minimas\n",
    "from scipy.signal import argrelextrema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is partially a dummy function. This extraction is limited by the fact that XML isn't standard. \n",
    "However, I want to start parsing and iterating over txt using GPT as a way of mechanizing/beginning to evaluate our \n",
    "thoughts on how to gain greater info about these papers. \n",
    "\"\"\"\n",
    "def splitXMLParagraphs(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Passing the stored data inside\n",
    "    # the beautifulsoup parser, storing\n",
    "    # the returned object\n",
    "    Bs_data = BeautifulSoup(data, \"xml\")\n",
    "    para = Bs_data.find_all('p')\n",
    "    paragraphs = []\n",
    "\n",
    "    for x in range(len(para)): \n",
    "        if len(para[x].text) < 2800:\n",
    "            paragraphs.append(para[x].text)\n",
    "        else: \n",
    "            para[x.text].split\n",
    "            x -= 1\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'format': 'PDF 1.4',\n",
       " 'title': 'doi:10.1016/j.snb.2008.10.030',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'Elsevier',\n",
       " 'producer': 'Acrobat Distiller 7.0 (Windows)',\n",
       " 'creationDate': 'D:20090112130006Z',\n",
       " 'modDate': \"D:20090117135818+05'30'\",\n",
       " 'trapped': '',\n",
       " 'encryption': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pdfMetadata(filepath): \n",
    "    doc = fitz.open(filepath)\n",
    "    metadata = doc.metadata\n",
    "    return metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfTextExtraction(filename, filepath): \n",
    "    doc = fitz.open(filepath)  # open document\n",
    "    with open(filename + '.txt', 'w') as out:\n",
    "        for page in doc:  # iterate the document pages\n",
    "            text = page.get_text().encode(\"utf8\")  # get plain text (is in UTF-8)\n",
    "            out.write(text.decode('utf-8'))  # write text of page\n",
    "            out.write(bytes((12,)).decode('utf-8'))  # write page delimiter (form feed 0x0C) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a bunch of code from This guy -> https://medium.com/@npolovinkin/how-to-chunk-text-into-paragraphs-using-python-8ae66be38ea6\n",
    "def rev_sigmoid(x:float)->float:\n",
    "    return (1 / (1 + math.exp(0.5*x)))\n",
    "    \n",
    "def activate_similarities(similarities:np.array, p_size=10)->np.array:\n",
    "    \"\"\" Function returns list of weighted sums of activated sentence similarities\n",
    "\n",
    "    Args:\n",
    "        similarities (numpy array): it should square matrix where each sentence corresponds to another with cosine similarity\n",
    "        p_size (int): number of sentences are used to calculate weighted sum \n",
    "\n",
    "    Returns:\n",
    "        list: list of weighted sums\n",
    "    \"\"\"\n",
    "    # To create weights for sigmoid function we first have to create space. P_size will determine number of sentences used and the size of weights vector.\n",
    "    x = np.linspace(-10,10,p_size)\n",
    "    # Then we need to apply activation function to the created space\n",
    "    y = np.vectorize(rev_sigmoid) \n",
    "        # Because we only apply activation to p_size number of sentences we have to add zeros to neglect the effect of every additional sentence and to match the length ofvector we will multiply\n",
    "    activation_weights = np.pad(y(x),(0,similarities.shape[0]-p_size))\n",
    "    ### 1. Take each diagonal to the right of the main diagonal\n",
    "    diagonals = [similarities.diagonal(each) for each in range(0,similarities.shape[0])]\n",
    "    ### 2. Pad each diagonal by zeros at the end. Because each diagonal is different length we should pad it with zeros at the end\n",
    "    diagonals = [np.pad(each, (0,similarities.shape[0]-len(each))) for each in diagonals]\n",
    "    ### 3. Stack those diagonals into new matrix\n",
    "    diagonals = np.stack(diagonals)\n",
    "    ### 4. Apply activation weights to each row. Multiply similarities with our activation.\n",
    "    diagonals = diagonals * activation_weights.reshape(-1,1)\n",
    "    ### 5. Calculate the weighted sum of activated similarities\n",
    "    activated_similarities = np.sum(diagonals, axis=0)\n",
    "    return activated_similarities\n",
    "  \n",
    "\n",
    "def CreateModularContent(path, fname, sentencetransformer):\n",
    "    \"\"\" Function returns a list of paragraphs from a pdf\n",
    "\n",
    "    Args:\n",
    "        path (string): the file path to the PDF in concern\n",
    "        fname (string): file name of the pdf\n",
    "        sentencetransformer (sentencetransformer instance): Takes an instance of the sentence transformer library\n",
    "\n",
    "    Returns:\n",
    "        paragraphs: list of paragraphs in the file\n",
    "    \"\"\"\n",
    "    #reading the desired file\n",
    "    with open(path+fname + \".txt\", 'r') as file:\n",
    "     contents = file.read()\n",
    "\n",
    "    #separating the file into an array based on when there are periods. \n",
    "    list_of_contents = contents.split(\".\")\n",
    "    embeddings = sentencetransformer.encode(list_of_contents)\n",
    "\n",
    "        \n",
    "    # Create similarities matrix\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Lets apply activated_similarities. For long sentences i reccomend to use 10 or more sentences (not sure what p_size does)\n",
    "    activated_similarities = activate_similarities(similarities, p_size=similarities.shape[0])\n",
    "\n",
    "    ### 6. Find relative minima of our vector. For all local minimas and save them to variable with argrelextrema function\n",
    "    minmimas = argrelextrema(activated_similarities, np.less, order=2) #order parameter controls how frequent should be splits. I would not reccomend changing this parameter.\n",
    "    # plot the flow of our text with activated similarities\n",
    "\n",
    "    #visualization stuff that we don't need \n",
    "    # lets create empty fig for our plor\n",
    "    #fig, ax = plt.subplots()\n",
    "    #sns.lineplot(y=activated_similarities, x=range(len(activated_similarities)), ax=ax).set_title('Relative minimas');\n",
    "    # Now lets plot vertical lines in order to see where we created the split\n",
    "    #plt.vlines(x=minmimas, ymin=min(activated_similarities), ymax=max(activated_similarities), colors='purple', ls='--', lw=1, label='vline_multiple - full height')\n",
    "\n",
    "    #Get the order number of the sentences which are in splitting points\n",
    "    split_points = [each for each in minmimas[0]]\n",
    "    # Create empty string\n",
    "    text = ''\n",
    "    for num,each in enumerate(list_of_contents):\n",
    "        # Check if sentence is a minima (splitting point)\n",
    "        if num in split_points:\n",
    "            # If it is than add a dot to the end of the sentence and a paragraph before it.\n",
    "            text+=f'\\n{each}. '\n",
    "        else:\n",
    "            # If it is a normal sentence just add a dot to the end and keep adding sentences.\n",
    "            text+=f'{each}. '\n",
    "   \n",
    "    with open(path + fname + \"_para\" + \".txt\", 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    paragraphs = f.readline()\n",
    "    return paragraphs   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limitation -> recipe for paragraph extraction, but not necessarily basic metadata like authors etc\n",
    "def contentMetadataRecipe(openai, filename, prompt): \n",
    "# Imports GPT3 model. Using davinci at the moment for final outputs. Curie for testing. \n",
    "    counter = 0\n",
    "    res = \"\"\n",
    "    ans = []\n",
    "    #Wondering if we can retrieve the model earlier on -> so we don't have to do this multiple times. \n",
    "    #openai.Model.retrieve(\"text-curie-001\")\n",
    "    openai.Model.retrieve(\"text-curie-001\")\n",
    "    \n",
    "    with open(filename, 'r') as f: \n",
    "        paragraphs = f.readlines()\n",
    "\n",
    "    # structures the base prompt for the model\n",
    "    #TO BE UPDATED. I want to train my own version of this. \n",
    "    #base_prompt = \"Paragraph:So yeah, do you see in those ecosystems really cool as pop in? Lots of cool projects, many more I forgot a bunch, but yeah, Jocelyn is always curating this cool landscape, so just check it out. I have the Twitter right there. And yeah, so we just heard about it. So sharing scientific data is super important. Why? Because, well, if we share data, we can collaborate much more easily. We can build bigger data sets and bigger data sets means more statistical power, reliable results, right? So that's pretty cool. And it also means more access to the data that, so there's not the same access to cool instruments that help you with data collection across labs. So if you're in an underfunded research institution, you just may not have the ability to collect the same type of data that a well-funded institution may have. So if we all share data, we all have better access to make cool scientific discoveries. So that's pretty cool, right? But also sharing scientific data right now. It's pretty expensive, it's pretty vulnerable because it's stored on centralized databases where we just have to trust that they keep the database running. It's also not rewarded. So currently, what counts in science is having your PDF cited, but it doesn't matter if you make your data accessible, like you just cannot accrue credit to it. Or there's some ways you can, but it's just not really easy. And it's also pretty painful. So there's a couple of repos out there where you can store your data. These are funded by some governmental institutions. There you access not great. And then also, if you want to find the data, you need to know which repo it's stored at. So you need to find the repo. Then you need to find the data. It's all, it's a hassle, so it's not great.\\nExample Summary:Sharing scientific data is important as it allows for better collaboration, bigger data sets, reliable results, and better access for researchers in underfunded institutions. However, currently sharing data is expensive, vulnerable, and not rewarded. It is stored on centralized databases which requires that we trust those servers to keep running. Also, there are no incentives for for making the data accessible. Currently, the only way that we can give credit for using someone else's work is citing their PDF. But with PDF citations, it doesn't matter if you make your data accessible. Sharing data right now isn't worth the cost and time for the researcher.\\nParagraph:\"\n",
    "\n",
    "    base_prompt = \"Does this paragraph describe the paper's \" + prompt + \"Answer with a Yes or No.\"\n",
    "    \n",
    "    for x in range(len(paragraphs)): \n",
    "        if len(paragraphs[x]) > 100: \n",
    "            thought = paragraphs[x].strip()\n",
    "            p = base_prompt + thought\n",
    "        else: \n",
    "            continue\n",
    "        \n",
    "        print('I enter the loop when my paragraph is as tiny as:' +str(len(paragraphs[x])))\n",
    "        # Model parameters were determined through sandbox testing. Temp is fairly high to allow the model\n",
    "        response = openai.Completion.create(\n",
    "            #model = \"text-curie-001\",\n",
    "            model=\"text-curie-001\",\n",
    "            prompt = p,\n",
    "            max_tokens=400,\n",
    "            temperature=0.7,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0.5,\n",
    "            presence_penalty=0.5\n",
    "        )\n",
    "        answer = response[\"choices\"][0][\"text\"]\n",
    "        ans.append(answer)\n",
    "        \n",
    "        if answer.find(\"Yes\") != -1: \n",
    "            question = \"What is the paper's\" + prompt + \"?\" + thought\n",
    "            # Model parameters were determined through sandbox testing. Temp is fairly high to allow the model\n",
    "            response2 = openai.Completion.create(\n",
    "                #model = \"text-curie-001\",\n",
    "                model=\"text-curie-001\",\n",
    "                prompt = question,\n",
    "                max_tokens=400,\n",
    "                temperature=0.7,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0.5,\n",
    "                presence_penalty=0.5\n",
    "            )\n",
    "            res += response2[\"choices\"][0][\"text\"]    \n",
    "        \n",
    "        \n",
    "        counter+=1\n",
    "        #print(counter)\n",
    "        # A sleep counter because microsoft keeps limiting my creativity\n",
    "        if counter%30==0 and counter!=0:\n",
    "            print(\"\\n\\n\\nI am so sleepy\\n\\n\\n\")\n",
    "            time.sleep(60)\n",
    "        \n",
    "        if len(res) < 1600: \n",
    "            final = \"Summarize these responses into one sentence that tells me the paper's\" + prompt + \"\\n\" + res\n",
    "            response3 = openai.Completion.create(\n",
    "                        #model = \"text-curie-001\",\n",
    "                        model=\"text-curie-001\",\n",
    "                        prompt = final,\n",
    "                        max_tokens=400,\n",
    "                        temperature=0.7,\n",
    "                        top_p=1,\n",
    "                        frequency_penalty=0.5,\n",
    "                        presence_penalty=0.5\n",
    "                )\n",
    "        else:\n",
    "            modular = res.split('.')\n",
    "            for i in range(len(modular)): \n",
    "                if i< len(modular)/2:\n",
    "                    res1 += modular[i]\n",
    "                else:\n",
    "                    res2 += modular[i]\n",
    "            \n",
    "            if len(res1) < 1600: \n",
    "                final = \"Summarize this responses into one sentence that tells me the paper's\" + prompt + \"\\n\" + res1\n",
    "                response4 = openai.Completion.create(\n",
    "                            #model = \"text-curie-001\",\n",
    "                            model=\"text-curie-001\",\n",
    "                            prompt = final,\n",
    "                            max_tokens=400,\n",
    "                            temperature=0.7,\n",
    "                            top_p=1,\n",
    "                            frequency_penalty=0.5,\n",
    "                            presence_penalty=0.5\n",
    "                    )\n",
    "            if len(res2) < 1600: \n",
    "                final = \"Summarize this responses into one sentence that tells me the paper's\" + prompt + \"\\n\" + res2\n",
    "                response5 = openai.Completion.create(\n",
    "                            #model = \"text-curie-001\",\n",
    "                            model=\"text-curie-001\",\n",
    "                            prompt = final,\n",
    "                            max_tokens=400,\n",
    "                            temperature=0.7,\n",
    "                            top_p=1,\n",
    "                            frequency_penalty=0.5,\n",
    "                            presence_penalty=0.5\n",
    "                    )\n",
    "\n",
    "    return(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Research Question': 'this is the research question'}\n",
      "I enter the loop when my paragraph is as tiny as:119\n",
      "I enter the loop when my paragraph is as tiny as:102\n",
      "I enter the loop when my paragraph is as tiny as:150\n",
      "I enter the loop when my paragraph is as tiny as:147\n",
      "I enter the loop when my paragraph is as tiny as:145\n",
      "I enter the loop when my paragraph is as tiny as:146\n",
      "I enter the loop when my paragraph is as tiny as:146\n",
      "I enter the loop when my paragraph is as tiny as:147\n",
      "I enter the loop when my paragraph is as tiny as:148\n",
      "I enter the loop when my paragraph is as tiny as:153\n",
      "I enter the loop when my paragraph is as tiny as:142\n",
      "I enter the loop when my paragraph is as tiny as:149\n",
      "I enter the loop when my paragraph is as tiny as:151\n",
      "I enter the loop when my paragraph is as tiny as:147\n",
      "I enter the loop when my paragraph is as tiny as:147\n",
      "I enter the loop when my paragraph is as tiny as:123\n",
      "I enter the loop when my paragraph is as tiny as:202\n",
      "I enter the loop when my paragraph is as tiny as:202\n",
      "I enter the loop when my paragraph is as tiny as:184\n",
      "I enter the loop when my paragraph is as tiny as:178\n",
      "I enter the loop when my paragraph is as tiny as:167\n",
      "I enter the loop when my paragraph is as tiny as:158\n",
      "I enter the loop when my paragraph is as tiny as:167\n",
      "I enter the loop when my paragraph is as tiny as:164\n",
      "I enter the loop when my paragraph is as tiny as:155\n",
      "I enter the loop when my paragraph is as tiny as:166\n",
      "I enter the loop when my paragraph is as tiny as:156\n",
      "I enter the loop when my paragraph is as tiny as:188\n",
      "I enter the loop when my paragraph is as tiny as:184\n",
      "I enter the loop when my paragraph is as tiny as:177\n",
      "\n",
      "\n",
      "\n",
      "I am so sleepy\n",
      "\n",
      "\n",
      "\n",
      "I enter the loop when my paragraph is as tiny as:176\n",
      "I enter the loop when my paragraph is as tiny as:174\n",
      "I enter the loop when my paragraph is as tiny as:198\n",
      "I enter the loop when my paragraph is as tiny as:191\n",
      "I enter the loop when my paragraph is as tiny as:188\n",
      "I enter the loop when my paragraph is as tiny as:188\n",
      "I enter the loop when my paragraph is as tiny as:188\n",
      "I enter the loop when my paragraph is as tiny as:164\n",
      "I enter the loop when my paragraph is as tiny as:167\n",
      "I enter the loop when my paragraph is as tiny as:169\n",
      "I enter the loop when my paragraph is as tiny as:175\n",
      "I enter the loop when my paragraph is as tiny as:188\n",
      "I enter the loop when my paragraph is as tiny as:192\n",
      "I enter the loop when my paragraph is as tiny as:184\n",
      "I enter the loop when my paragraph is as tiny as:163\n",
      "I enter the loop when my paragraph is as tiny as:175\n",
      "I enter the loop when my paragraph is as tiny as:185\n",
      "I enter the loop when my paragraph is as tiny as:195\n",
      "I enter the loop when my paragraph is as tiny as:191\n",
      "I enter the loop when my paragraph is as tiny as:165\n",
      "I enter the loop when my paragraph is as tiny as:168\n",
      "I enter the loop when my paragraph is as tiny as:172\n",
      "I enter the loop when my paragraph is as tiny as:160\n",
      "I enter the loop when my paragraph is as tiny as:177\n",
      "I enter the loop when my paragraph is as tiny as:166\n",
      "I enter the loop when my paragraph is as tiny as:161\n",
      "I enter the loop when my paragraph is as tiny as:183\n",
      "I enter the loop when my paragraph is as tiny as:158\n",
      "I enter the loop when my paragraph is as tiny as:153\n",
      "I enter the loop when my paragraph is as tiny as:152\n",
      "\n",
      "\n",
      "\n",
      "I am so sleepy\n",
      "\n",
      "\n",
      "\n",
      "I enter the loop when my paragraph is as tiny as:188\n",
      "I enter the loop when my paragraph is as tiny as:167\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 2049 tokens, however you requested 2059 tokens (1659 in your prompt; 400 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmetadata.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m write_file:\n\u001b[1;32m     36\u001b[0m         json\u001b[39m.\u001b[39mdump(metadata, write_file, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)  \n\u001b[0;32m---> 38\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[40], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(categories)):\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(contentMetadata)\n\u001b[0;32m---> 29\u001b[0m     contentMetadata \u001b[39m=\u001b[39m {categories[i]: contentMetadataRecipe(openai, filepathtxt, categories[i])}\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(contentMetadata)\n\u001b[1;32m     33\u001b[0m metadata \u001b[39m=\u001b[39m [contentMetadata, descriptiveMetadata]\n",
      "Cell \u001b[0;32mIn[39], line 67\u001b[0m, in \u001b[0;36mcontentMetadataRecipe\u001b[0;34m(openai, filename, prompt)\u001b[0m\n\u001b[1;32m     63\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m60\u001b[39m)\n\u001b[1;32m     65\u001b[0m     final \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSummarize these responses into one sentence that tells me the paper\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m res\n\u001b[0;32m---> 67\u001b[0m     response3 \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     68\u001b[0m                 \u001b[39m#model = \"text-curie-001\",\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m                 model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-curie-001\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     70\u001b[0m                 prompt \u001b[39m=\u001b[39;49m final,\n\u001b[1;32m     71\u001b[0m                 max_tokens\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m,\n\u001b[1;32m     72\u001b[0m                 temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m     73\u001b[0m                 top_p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     74\u001b[0m                 frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     75\u001b[0m                 presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m(response3)\n",
      "File \u001b[0;32m~/Documents/venv/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Documents/venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Documents/venv/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/Documents/venv/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/venv/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 2049 tokens, however you requested 2059 tokens (1659 in your prompt; 400 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    \"\"\"\n",
    "    A potential combo of the functions above to get a set of metadata out. \n",
    "    \"\"\"\n",
    "    openai.api_key = \"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\"\n",
    "\n",
    "    filename = 'Papageorgiou et al_2017_Mechanical properties of graphene and graphene-based nanocomposites'\n",
    "\n",
    "    filepathpdf = \"/Users/desot1/Dev/desci/Papageorgiou et al_2017_Mechanical properties of graphene and graphene-based nanocomposites.pdf\"\n",
    "    \n",
    "    text = pdfTextExtraction('Papageorgiou et al_2017_Mechanical properties of graphene and graphene-based nanocomposites.txt', filepathpdf)\n",
    "    \n",
    "    filepathtxt = filename + '.txt'\n",
    "\n",
    "    descriptiveMetadata = pdfMetadata(filepathpdf)\n",
    "    \n",
    "    contentMetadata = {}\n",
    "\n",
    "\n",
    "    categories = ['Research Question', 'Alterative Approaches', 'Hypothesis', 'Methodology', 'Results', 'Inferences']\n",
    "    \n",
    "    #paragraphs = splitXMLParagraphs(filepathxml)\n",
    "    contentMetadata['Research Question'] = 'this is the research question'\n",
    "\n",
    "\n",
    "    for i in range(len(categories)):\n",
    "        print(contentMetadata)\n",
    "\n",
    "        contentMetadata = {categories[i]: contentMetadataRecipe(openai, filepathtxt, categories[i])}\n",
    "        \n",
    "    print(contentMetadata)\n",
    "\n",
    "    metadata = [contentMetadata, descriptiveMetadata]\n",
    "\n",
    "    with open(\"metadata.json\", \"w\") as write_file:\n",
    "        json.dump(metadata, write_file, indent=4)  \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
