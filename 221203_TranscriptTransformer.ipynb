{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# First let import the most necessary libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import openai as openai\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import whisper\n",
    "\n",
    "# Library to import pre-trained model for sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Calculate similarities between sentences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization library\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# package for finding local minimas\n",
    "from scipy.signal import argrelextrema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code takes in the audio file and writes it to a txt file. \n",
    "\n",
    "def GetTranscriptFromAudio(path, audiofilename, type):\n",
    "\n",
    "    # Setup, define, and run Whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "    input = path + audiofilename + type\n",
    "    output = audiofilename + \".txt\"\n",
    "    \n",
    "    with open(path + output, 'w') as file:\n",
    "     file.write(model.transcribe(input)[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the text input from the youtube video and uses GPT3 to turn it into a cohesive \n",
    "# series of thoughts. Those can later be turned into a twitter thread\n",
    "\n",
    "\n",
    "#a bunch of code from This guy -> https://medium.com/@npolovinkin/how-to-chunk-text-into-paragraphs-using-python-8ae66be38ea6\n",
    "def rev_sigmoid(x:float)->float:\n",
    "    return (1 / (1 + math.exp(0.5*x)))\n",
    "    \n",
    "def activate_similarities(similarities:np.array, p_size=10)->np.array:\n",
    "    \"\"\" Function returns list of weighted sums of activated sentence similarities\n",
    "\n",
    "    Args:\n",
    "        similarities (numpy array): it should square matrix where each sentence corresponds to another with cosine similarity\n",
    "        p_size (int): number of sentences are used to calculate weighted sum \n",
    "\n",
    "    Returns:\n",
    "        list: list of weighted sums\n",
    "    \"\"\"\n",
    "    # To create weights for sigmoid function we first have to create space. P_size will determine number of sentences used and the size of weights vector.\n",
    "    x = np.linspace(-10,10,p_size)\n",
    "    # Then we need to apply activation function to the created space\n",
    "    y = np.vectorize(rev_sigmoid) \n",
    "        # Because we only apply activation to p_size number of sentences we have to add zeros to neglect the effect of every additional sentence and to match the length ofvector we will multiply\n",
    "    activation_weights = np.pad(y(x),(0,similarities.shape[0]-p_size))\n",
    "    ### 1. Take each diagonal to the right of the main diagonal\n",
    "    diagonals = [similarities.diagonal(each) for each in range(0,similarities.shape[0])]\n",
    "    ### 2. Pad each diagonal by zeros at the end. Because each diagonal is different length we should pad it with zeros at the end\n",
    "    diagonals = [np.pad(each, (0,similarities.shape[0]-len(each))) for each in diagonals]\n",
    "    ### 3. Stack those diagonals into new matrix\n",
    "    diagonals = np.stack(diagonals)\n",
    "    ### 4. Apply activation weights to each row. Multiply similarities with our activation.\n",
    "    diagonals = diagonals * activation_weights.reshape(-1,1)\n",
    "    ### 5. Calculate the weighted sum of activated similarities\n",
    "    activated_similarities = np.sum(diagonals, axis=0)\n",
    "    return activated_similarities\n",
    "  \n",
    "\n",
    "def CreateModularContent(path, transcript, sentencetransformer):\n",
    "    #reading the desired file\n",
    "    with open(path+transcript + \".txt\", 'r') as file:\n",
    "     contents = file.read()\n",
    "    \n",
    "    contents = contents.replace(\"?\", \".\")\n",
    "    \n",
    "    #separating the file into an array based on when there are periods. \n",
    "    list_of_contents = contents.split(\".\")\n",
    "    \n",
    "    # Get the length of each sentence\n",
    "    sentence_length = [len(each) for each in list_of_contents]\n",
    "    \n",
    "    \n",
    "    # Determine longest outlier\n",
    "    long = np.mean(sentence_length) + np.std(sentence_length) *2\n",
    "    \n",
    "    # Determine shortest outlier\n",
    "    short = 20 #np.mean(sentence_length) - np.std(sentence_length) *2\n",
    "    \n",
    "    \n",
    "    # Shorten long sentences\n",
    "    text = ''\n",
    "    for each in list_of_contents:\n",
    "        if len(each) > long:\n",
    "            # let's replace all the commas with dots\n",
    "            each.replace(',', '.') \n",
    "        else:\n",
    "            text+= f'{each}.'\n",
    "    count = 0        \n",
    "    list_of_contents = text.split('.')\n",
    "    \n",
    "    # Now let's concatenate short ones\n",
    "    text = ''\n",
    "    \n",
    "    for each in list_of_contents:\n",
    "        if len(each) < short:\n",
    "            text+= f'{each}'\n",
    "        else:\n",
    "            text+= f'{each}.'\n",
    "    \n",
    "    list_of_contents = text.split(\".\")\n",
    "    \n",
    "    \n",
    "    embeddings = sentencetransformer.encode(list_of_contents)\n",
    "\n",
    "        \n",
    "    # Create similarities matrix\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Lets apply activated_similarities. For long sentences i reccomend to use 10 or more sentences (not sure what p_size does)\n",
    "    activated_similarities = activate_similarities(similarities, p_size=similarities.shape[0])\n",
    "\n",
    "    ### 6. Find relative minima of our vector. For all local minimas and save them to variable with argrelextrema function\n",
    "    minmimas = argrelextrema(activated_similarities, np.less, order=2) #order parameter controls how frequent should be splits. I would not reccomend changing this parameter.\n",
    "    # plot the flow of our text with activated similarities\n",
    "\n",
    "    #Get the order number of the sentences which are in splitting points\n",
    "    split_points = [each for each in minmimas[0]]\n",
    "   \n",
    "   # Create empty string\n",
    "    text = ''\n",
    "    for num,each in enumerate(list_of_contents):\n",
    "        # Check if sentence is a minima (splitting point)\n",
    "        if num in split_points:\n",
    "            # If it is than add a dot to the end of the sentence and a paragraph before it.\n",
    "            text+=f'\\n {each}.'\n",
    "        else:\n",
    "            # If it is a normal sentence just add a dot to the end and keep adding sentences.\n",
    "            text+=f'{each}. '\n",
    "   \n",
    "    with open(path + transcript + \"_modular\" + \".txt\", 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return transcript + \"_modular\"   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(raw_file, agg):\n",
    "    thought = raw_file.readline()\n",
    "    for i in range(agg-1):\n",
    "        if raw_file.readline() == \"\": \n",
    "            agg += 1\n",
    "        else:      \n",
    "            thought += raw_file.readline()\n",
    "    return thought\n",
    "\n",
    "def SummarizeThoughts(path, filename, openai, agg, iterations): \n",
    "# Imports GPT3 model. Using davinci at the moment for final outputs. Curie for testing. \n",
    "\n",
    "    #Wondering if we can retrieve the model earlier on -> so we don't have to do this multiple times. \n",
    "    #openai.Model.retrieve(\"text-curie-001\")\n",
    "    openai.Model.retrieve(\"text-davinci-002\")\n",
    "    limcurie = 2040\n",
    "    limdavinci = 2040\n",
    "\n",
    "    #File management. \n",
    "    cleaned_file = open(path + filename + \"_\" + str(iterations) + \".txt\", \"w\")\n",
    "    raw_file = open(path+filename + \".txt\")\n",
    "    thought = aggregate(raw_file, agg)\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    # structures the base prompt for the model\n",
    "    #TO BE UPDATED. I want to train my own version of this. \n",
    "    #base_prompt = \"Paragraph:So yeah, do you see in those ecosystems really cool as pop in? Lots of cool projects, many more I forgot a bunch, but yeah, Jocelyn is always curating this cool landscape, so just check it out. I have the Twitter right there. And yeah, so we just heard about it. So sharing scientific data is super important. Why? Because, well, if we share data, we can collaborate much more easily. We can build bigger data sets and bigger data sets means more statistical power, reliable results, right? So that's pretty cool. And it also means more access to the data that, so there's not the same access to cool instruments that help you with data collection across labs. So if you're in an underfunded research institution, you just may not have the ability to collect the same type of data that a well-funded institution may have. So if we all share data, we all have better access to make cool scientific discoveries. So that's pretty cool, right? But also sharing scientific data right now. It's pretty expensive, it's pretty vulnerable because it's stored on centralized databases where we just have to trust that they keep the database running. It's also not rewarded. So currently, what counts in science is having your PDF cited, but it doesn't matter if you make your data accessible, like you just cannot accrue credit to it. Or there's some ways you can, but it's just not really easy. And it's also pretty painful. So there's a couple of repos out there where you can store your data. These are funded by some governmental institutions. There you access not great. And then also, if you want to find the data, you need to know which repo it's stored at. So you need to find the repo. Then you need to find the data. It's all, it's a hassle, so it's not great.\\nExample Summary:Sharing scientific data is important as it allows for better collaboration, bigger data sets, reliable results, and better access for researchers in underfunded institutions. However, currently sharing data is expensive, vulnerable, and not rewarded. It is stored on centralized databases which requires that we trust those servers to keep running. Also, there are no incentives for for making the data accessible. Currently, the only way that we can give credit for using someone else's work is citing their PDF. But with PDF citations, it doesn't matter if you make your data accessible. Sharing data right now isn't worth the cost and time for the researcher.\\nParagraph:\"\n",
    "\n",
    "    base_prompt = \"Paragraph:In particular the last five years has been full time at this organization called the GoFair Foundation. These slides are available. differences that are in here too. Okay, so what is this idea going fair and doing fair? So this presentation will be in two parts and the first part going fair is really about kind of the history of where this idea came from about fair and kind of how it's matured up until today and then in the next section on doing fair, just to kind of give you a snapshot of some state of the art developments right now on the implementation of the fair principles and kind of, you know, the doors that opens up to kind of a new way of looking at computing and a new way of doing information exchange and being able to access larger amounts of data for kind of, you know, distributed learning type analyses.\\nSummary:What is going and doing fair? We’ll break this down into two parts - first we’ll explore the history of fair and how it has matured until today, then we’ll explore the implementation of the Fair principles as a novel method of computing, information exchange, and distributed learning analysis.\\nParagraph:Okay, so I'm going to start here with a picture from, that I took in 2019 in the gentleman at the podium, his name is George Straun and in 1995, he was the director of something called the NSF Net, the National Science Foundation Network and would George is saying at this, in this presentation here is that in 2019, the internet was now 50 years old. So in particular, what he's really referring to is that this key technology of the modern internet, TCP IP was invented in 1969 and underwent a lot of research and development for 20 years. Then there was another 10 years where TCP IP was used to implement the NSF Net and the goal of the project was to connect, I think, 100 American universities to the national supercomputing centers that had grown up in the United States. And it's kind of strange to think about it now, but just the idea that there were local computing networks and if you wanted to interconnect them, that that was a real engineering problem. It was interoperability of the networks. And so this TCP IP was helping to create what they call the internet, the interoperable network. But it was, you know, at that time, a real engineering problem, right?\\nSummary:In 1995, George Straun was the director of the National Science Foundation Network (NSF Net). And the goal of the NSF at that time was to connect 100 American Universities to the national supercomputing centers. Now, while this seems simple now the key technology of the internet, TCP IP, was invented just 26 years earlier in 1969, and finding a way to connect many local networks running on this invention was a huge engineering challenge. They were asking the question, how do we make a global internet? An interoperable network.\\nParagraph:\"\n",
    "    while thought:\n",
    "        # Clean off white space for OpenAI\n",
    "        thought=thought.strip()\n",
    "        #if len(thought) > limcurie: \n",
    "            #split the thought preferebly. \n",
    "                #\n",
    "\n",
    "        # Prepare full prompt\n",
    "        p = base_prompt + thought + \"\\nSummary:\"\n",
    "\n",
    "        # Model parameters were determined through sandbox testing. Temp is fairly high to allow the model\n",
    "        response = openai.Completion.create(\n",
    "            #note on this -> I'm suspicious -> why are we defining this twice? Play around with cleaning this.\n",
    "            #model = \"text-curie-001\",\n",
    "            model=\"text-davinci-002\",\n",
    "            prompt = p,\n",
    "            max_tokens=400,\n",
    "            temperature=0.7,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0.5,\n",
    "            presence_penalty=0.5\n",
    "        ) \n",
    "\n",
    "        # answer logging and .txt formatting\n",
    "        cleaned_file.write(response[\"choices\"][0][\"text\"] + \"\\n\")\n",
    "\n",
    "        \n",
    "        counter+=1\n",
    "        print(counter)\n",
    "        # A sleep counter because microsoft keeps limiting my creativity\n",
    "        if counter%30==0 and counter!=0:\n",
    "            print(\"\\n\\n\\nI am so sleepy\\n\\n\\n\")\n",
    "            time.sleep(60)\n",
    "        \n",
    "        \n",
    "        thought = aggregate(raw_file, agg)\n",
    "\n",
    "    cleaned_file.close()\n",
    "    raw_file.close()\n",
    "    \n",
    "    #iterater\n",
    "    if iterations > 1: \n",
    "        filename = filename + \"_\" + str(iterations)\n",
    "        SummarizeThoughts(path, filename, openai, 3, iterations-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "\n",
      "\n",
      "\n",
      "I am so sleepy\n",
      "\n",
      "\n",
      "\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#This script should take you from an audio output to a TLDR type medium article in the \n",
    "#DeSci Foundation voice. \n",
    "\n",
    "#define the path you want materials to be saved too\n",
    "#export OPENAI_API_KEY=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\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "path = \"/Users/desot1/Documents/GitHub/DeSci-Experiments/\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "audiofile = \"BertAndErikConversationRecording\"\n",
    "type = \".m4a\"\n",
    "\n",
    "#openai api fine_tunes.create -t <train_file>\n",
    "\n",
    "\n",
    "#Step one is to transcribe the audio using Whisper. \n",
    "GetTranscriptFromAudio(path, audiofile, type)\n",
    "\n",
    "#Step two is to take the transcript and turn it into paragraphs\n",
    "modular = CreateModularContent(path, audiofile, model)\n",
    "\n",
    "#Step three is to take the modular content and summarize each paragraph \n",
    "SummarizeThoughts(path, modular, openai, 1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
