 All righty. Hello, hello. It is Eric and Bert and this conversation is going to be all about avoiding vendor lock-in for data storage. Sorry, Bert, I interrupted you just a second. That is perfect, Eric. Yes, we are very interested to make sure, to sure that when we see projects where data is exchanged, we have seen multiple parties that we do not see any form of vendor lock-in. We have seen a lot of vendor lock-in mainly in projects where different parties want to exchange data. And what we have seen is that in many cases there is one organization who says, we will take care of the data, just give the data to us and we will figure it out. We are not the way to do it. We really want to see that data exchange is as much as possible based on a federated decentralized structure. So where all the different parties involved keep their own databases, keep their own local systems, keep their own data models, and that the interoperability, the linking between the different systems and the different structures that these parties have, will actually be done at the metadata level. So not at the data level, but at the metadata level. So that means that if you, as an American, wants to record data in inches, buying, no problem, you know, I'm a Dutch guy, I like to measure in centimeters and that's also fine. As long as in the metadata, it's absolutely clear that you work in inches and I work in centimeters. Then, you know, we can exchange data to each other without any problems. So, vendor lock-in is an important element for us. Also, because we want to avoid that by having a strong vendor who is in charge of all the data, there is a monopoly created actually, and that for the different parties involved can be a problem at a later stage. Because then they have no longer control on their own data, and they are also dependent upon the central party who's then normally charging them money fees for their data. And they could double the price, and there's very little that these different participants of the data infrastructure can actually do. So, this is important for us. Well, you kind of touched on two different points around vendor lock-in that I would love to dive into separately. One of them being data lock-in through a lack of interoperability, or proprietary metadata formats. Another one being data lock-in through silos. Which one do you see being kind of the more problematic? Well, problematic is the wrong word. Which one do you see being a stronger lock-in? I think at the moment we are definitely seeing a lot of silos. You see that a lot of organizations who actually need to exchange data with each other are basically locked into silos. And it's very difficult for them to get out of that, to open that up, to communicate in the right way to other parties. Would you say this is the AWS kind of silo, as the stereotypical industry example? It is more the industry silos. In the industry, a lot of silos have been created simply by the fact that in the 1970s or in the 1980s or in the 1990s, certain systems were built. And based upon these systems, there's a certain critical mass actually using a certain vocabulary or a certain standard or a certain set of terms. That has deviated from maybe another entity or an organization or structure who has taken different decisions 10, 20, 30 years ago. And they've never been able to actually bring these two different worlds together. And they would love to see that the other one changes camp so that we all speak the same language. What you see here, and particularly you see that in medical care, we're also in, let's say, water management here in Holland, is that there's a certain attitude to impose systems and standards upon the others. And of course, when you look at the fair principles, it's actually that's a different way of looking at it. Fair principles actually say, you keep your standards. We're fine. You use what you want. And we can bridge that. And slowly we see that that attitude is resonating with people, but it's still a slow process because a lot of technical people, architects, et cetera, et cetera have been educated by trying to impose a certain standard upon a community. In Dutch, we actually talk about having one common language, having one common language in care, in medical care, in all kinds of service areas. And of course, what is disrupting and what is new about these fair principles is we say, you don't need this single language that everybody talks. You can still speak your own languages. No problem there. You can use your own standards. So that I think is the challenge to keep maybe even some silos in place. But to open them up by enriching the data with enough metadata, that for a machine, it's absolutely clear what is meant here. And I'm putting the emphasis on for the machine because every now and then we still see that people have the idea that these fair principles are for our own brains, you know, that is for us, for people. But in fact, it is not. It can help us as persons. But when you look at the fair principle, they were basically designed for the machines, for algorithms, for pieces of software, for queries, of course. So the idea is that data should be enriched with metadata in such a way that the machine can interpret the data in the correct way. And with all the aspects that are relevant for these datasets. Does that give you a feeling for what we're trying to achieve? Yeah, no, absolutely. And I do love this idea of a translation layer on top, especially given the fact that I would call a lot of the work that fair does, data, semantics. Every time I think about the work that I do, I tend to think about it in terms of language itself, actually. And it's a fun kind of parallel that can often be helpful in just allowing me to conceptualize some of these more concept or complex machine actionable concepts. Yeah, so the way that I'm thinking about this as you were talking about it, as someone who maybe hasn't been quite as steeped in the whole foundation, is, okay, we've got Dutch speakers, we've got English speakers, we've got Spanish speakers, we've got Chinese speakers. How do we make sure that all of these different people can communicate and work together in a reasonably efficient way? And in some instances, people are going to realize, hey, as a native Dutch speaker, I will be more professionally successful if I learn multiple languages. And we'll be willing to expand out past the existing standard. In some instances with English speakers, what you're going to have is absolutely no need outside, no need in the professional world to expand outside of what's there, but possibly a much better understanding of the world around you, if you cross pollinate ideas. And then you've got other domains, like let's say a country who's in Asia, who kind of has their own economy, their own ecosystem, their own language, their own culture, which doesn't have the same level of cross pollination that the US and Europe do, and it tends to be more on its own. So in essence, the way that I would see fair is as Google Translate for machines. I mean, that's basically what the entire system of ontologies are. And it's interesting how this spans across multiple levels as well, because you could see it in an industry by industry level. You can also see it within a specific subfield of science, where someone calls this genome one name, and then another scientist calls it another name. Realistically, it's the exact same like marker. There's no difference between the two, but just because we can't translate between the two of those, there is no way to efficiently transfer knowledge between different data sets. Right, there's no way to link them together. So it's interesting in both on an industry-wide level and on kind of a subfield level. And I would love to hear your take on more the human perspective of why some of these interoperability challenges happen in the first place, because to my understanding, for an individual scientist, they would name something, you know, their own name to say that they were the ones who discovered it. That it was me. I came up with this specific gene first. And if that name is the one that continues on, then my paper is the one that gets cited. And to a certain extent, it's the same thing with standards that we talked about above. But I would love to hear your take on both obviously that the machine portion of data silos, but also the human portion. That's why I think it has to go hand in glove, isn't it? It would be exclusively for machines. It would probably also be difficult because there's always the human element in exchanging data. But I like this model of the machine has to be able to interpret, you know, what is actually meant. Because we can all feel that if you enrich the data with so much, with the right metadata for the particular purposes purpose that a machine can understand it, then it's definitely also for us understandable. Yeah. Well, the other way around is not always the case. If you and I are fine with, you know, using a certain term, because we both know from a certain context, what is meant, it is not per se that the machine would also understand it. Because it may not have all the details about this context, because we've seen each other somewhere or we've been on the bike to tassel or to whatever. So our data exchange is of course always embedded in a certain context. And to make it explicit and absolutely clear and more objectively clear, this term, the machine has to be able to interpret and to understand what is meant is actually even stronger than what we are trying to exchange with each other. But, you know, when you add it all up, yes, it has to be clear for the machine, but it has to be clear for us as well. And not all the data maybe should be enriched with the same type of metadata. Obviously, you know, when you write a paper and you have a lot of data, you want to enrich the data at least with, you know, the person who did the exercise and where it was and the thunder and, you know, for which university it was and in which country that you did what. You know, all these type of things around the production of the paper itself, but also you want to be absolutely clear that, for instance, when you took certain tests that you want to put in the metadata under which conditions you took the test, where did you do the, did you do the mean a laboratory and under what circumstances was there. What that at a certain temperature, for instance, or what is a certain pressure, or what's, you know, all these type of technical data should also be there. So that for another researcher who will in a few months time study your data, he will recognize, oh, okay, I see, you know, this is how you did the test and this is how you did what. And that could explain maybe certain differences that this person has with his own data and the outcomes of his exercise. So it's this metadata is crucial in terms of the research itself, but also in terms of all the circumstances under which this is done. And that makes the research also stronger because then another person, first of all, can replicate the experiment that you've done. He can look at the difference with some of the experience that he's done himself. He can reach certain conclusions in terms of why is one experiment giving these results and another experiment giving that result. So it is certainly much more valuable and especially when it can be, I say, exchange with people all over the world. You can see that your research will then have much more impact than if you would just have done it for your own university and, you know, nobody could really do something smartly with that data. And I mean that this concept of interoperability, I think bringing it back down to a human level can also be very rewarding for scientists. Yes. One of the issues that I personally have with the larger system of science is the H index. It's the fact that everything is based on citation counts and that the moment you make one single metric, the single source of truth, the moment that a single source of truth exists in the first place, people will try to manipulate it. And scientists are very clever and have figured out how to manipulate the H index as they should. That's how you move your career forward. Absolutely. But one thing that's interesting to think about this is that a citation can be thought of as someone reusing an idea in one of your papers. A machine going through accessing, querying, and computing on your data can be thought of as a machine reusing one of your ideas. So it's actually a question of to me, how some of the underlying systems of credit within science start to change as we move into a digital age and the concept of interoperability itself, because ultimately scientists won't enter data until they get tenure for entering data. Like plain and simple, that's there has to be money tied to money prestige career. So, yeah, very true. And that's why it's very important that you also in the metadata give some very clear indications in terms of the provenance of the data. When you think about fair data, it is not per se that when I sent you a file, a data file in a fair form, about 100% fair, that it is a guarantee that all the data are also 100% correct. And it could very well have been that you've made this data up yourself or that there were mistakes made in the reading system of certain outcomes. So it's important that we know the provenance that we know where is this data coming from. Is it coming from North Korea, or is it coming from a very well established university in the states or in Germany? That makes a huge difference for some people in terms of how credible that data file is and how reusable that data file is also. Apart from the fairness, certain metadata elements could also give indications of the credibility. So, yeah, it is important for the humans also to be absolutely sure what is happening here. Where is this data coming from? I mean, the idea of creation, de-curation pipelines is something that I've been enthralled with for quite some time now. The idea that from the very moment that let's say a mass spec machine is taking a measurement, there is a cryptographic stamp that issues provenance. And then as it goes into its next phase of formatting, the compute that is run on that data, just to reformat it, has a stamp. And there's a difference, like a diff file, like a computer science style difference, from the mass spec file to the formatted file. And then let's say that formatted file goes through two to three different steps for analysis. Each one of those ties back in a tree to the original creation file, eventually you get to a point where data publishing happens. And I would say once you go from that creation to publishing and eventually with a human layer on top of it of curation, that is the complete flow of fare. And to me, it's not even just data provenance, it's compute provenance. I want to see all of the changes that have been made there. I want timestamps, I want cryptographic hashes for verifiability, a complete trail of everything that has been done is huge. And that's crucial because then you can really increase the trust level of data by magnitudes. So yes, it is very important to access it. And that's where fare actually meets block channel. Yeah, that's a part of it. Because you can immediately see, hey, if I have fair data and I can really look at the different steps of how the data was created by what was the process, how was it computed, who actually did what with the data. That can be an enormous, let's say, increase interest factor. And that's important because basically what you see is if you if you asked me why, why has, let's say, this whole concept of fair data, why is it relatively late when you compare to other data. Let's say, kind of think that we've seen in the last 20, 30 years, I would say it is the trust factor. There is very little trust in general in terms of data that has not been under our own control, that we have not produced ourselves, where we do not know exactly who's been involved in that whole process. So there's, there has always been a lot of mistrust or distrust in terms of sharing data, taking data from, from another party. And when you look at these fare principles, that at least is a chain or is a step in the right direction. When you look at block chain, it's also step in the right direction. So maybe we still have to make a few more steps in terms of getting really 100% trust in terms of data that is exchanging, especially when it's crucial data in terms of, you know, health of people or, you know, crucial data that could, let's say, change. Change, make very important changes in the lives of persons or in the security area or in, in safety areas. Bert, we should make a podcast. Yeah, no, I, I, I agree with you and it, I always draw this parallel and I would love to hear your take on it. I completely agree with everything that you just said. And this is a problem that was faced in the world of computer science at probably 20 to 30 years ago. All around the concept of waterfall methodologies and how we collaborate in terms of creating code, any kind of software development. Right. Early days, when people coded, they would save a file and send an email to their coworker and get something back. If you just got a random file in an email attachment, no provenance, no understanding of what it was, no nothing there. I mean, in this day and age, there's no way in hell I would open that under any circumstance. No, no circumstances. No, it's a terrible idea. Yeah, late 1990s, the Agile manifesto comes out, some of the concepts around like extreme programming and the Agile manifesto, well, the Agile manifesto was in 05. But no, I think it was a one. But this idea of small iterative steps of continuously showing your work as its own chain of trust started to become more realistic in the realm of software. In 2001, those principles were codified in the Agile manifesto, but this really took off in 2005 when Linus Torval created Git, the Git primitive as a means of handling, versioning and collaboration and software development. 2008 GitHub was released, I think like 2011 Microsoft bought it for like seven and a half billion dollars. But if you look into a GitHub repository, every single push that has ever been made, every pull request, every fork, every like badge and attestation and starts, all there is a little bit. So you can understand like if I go on to a specific GitHub repository, I can see if it was one single person who just published everything all at once, nobody is like this repo, nobody has started this repo. So like I can look into bits of the code and say, okay, yeah, this does not seem to be formatted correctly. I'm not going to use this. Whereas I go to another repository, it's got 75 stars. It's got 25 different contributors. It has had a consistent number of pushes over a period of time. There's a wide variety of metrics around trust. And ultimately what that comes from in my opinion is the tooling workflows and culture needed for transparency and collaboration and good lord that's into Ted a lot of buzzwords and I apologize. I mean, there's so much overlap. And if we start to think about what the agile manifesto mapped to science actually looks like and how detailed provenance because software development is one thing, science is another. Yeah, we're in software development. If you have two different functions that accomplish the same goal, yeah, it's fine, whatever, don't care. In science, that's not necessarily always the case near same lids are a very serious problem. So thinking about how the agile manifesto maps to science is a very interesting question around how we establish provenance and work to bust some of these more culturally induced data silos. Yeah, absolutely, absolutely. So yeah, it's critical that you know the whole chain of the provenance. You know, it should be recorded in such a way that we can find it and that we can and that we can interpret it. Yeah, absolutely, but it's also it's a challenging question for any of this provenance because anyone who holds the record of provenance becomes the single source of truth. And that's problematic. Once again, going back to this idea of the H index of a single source of truth for what quality is, well, a single source of truth for what history is also an issue. Yeah, so if we want to have this provenance, not only do we need to be able to trust the trail, we need to be able to trust the platform that this trail sits on. To me, that really is where blockchain comes into this entire system. It is a and I hate the term trustless systems. I'm sorry for using this, but it is a trustless system for scientific profits in my opinion. Definitely, definitely, but you know, my feeling is that over the next couple of years, we will make a few more steps in the right direction. We see now everywhere that these fair principles are pretty well accepted at all kind of levels. It's still a bit of a difficult journey that we're having at this moment. And that is because on the one hand, it's relatively easy to explain. Yeah, you can explain it in two to three minutes and people will grasp it. And on the other hand, it's relatively complex to implement it. Because you really need, you need to change your processes internally and you really need some data stewards who actually know what they're doing in terms of ensuring that, you know, your data actually gets a level of fairness that is acceptable and that is right for this interoperability. What we've seen recently also is that there's a lot of what we call pseudo-fair. These are people who say, what do you mean paired with fair? I was already fair in 1974 because I had a nice billboard. You could find me. I don't know what you mean with interoperability, but at least I was reusable. I was reusable at that time. So I was already fair for 30, 40 years ago. And we even see organizations at this moment claiming fair because they think that having a nice website and a DOI that is the Holy Grail for them. And of course, it is not. It may be a small step in the right direction, but it is definitely not fairness the way that we see it. So we have to be very careful also in terms of allowing organizations to just shout that they are fair and constantly looking under the hood. What is really happening and then telling them, no, no, nice attempt, but this is not what we mean with a full fair set of data. You should add this, this, that. I've been involved in a project in the Nordic countries that was financed by the European Open Science Cloud. And it was exactly what we were doing there. There were a large number of repositories who showed their data, came with their data. We had some evaluated software running where we could actually guide them into the right directions and saying, well, you know, here are a few elements that you can actually add to your data to improve that level of fairness. And you know, it took us, what is it? Maybe 12 or 15 months or so over a number of recurring webinars and workshops to actually get these repositories to the right level. So what I said on the one hand is really, it's really easy to explain fair and on the other end, it really takes an effort, it takes an effort to get there. And that is the challenge also. But I've already seen it in the last three to four years. We've made enormous steps. I think the challenge now is to train more people in this whole area who can then help organizations to make the right steps forward. To me, one of the more impressive things about fair is the way that it really has caught on as a grassroots movement in academia. I've been to academic conferences through desilabs. And I've had people come up, like we had fair on the sign behind us. I've had people come up and start talking to me about this. Like it's been very impressive to watch. And ultimately, that is both the hardest thing to get is that kind of grassroots ethos. That is the hardest thing to get and the best thing that you can possibly have. Because let's say one of these large generalist repositories, I was actually on their website about two days ago. No, it was yesterday. And I saw where they under their principles had the fair principles listed and had answers for how they solved every single one of these 15 points. And like seven of them were wrong. Seven of them were just objectively wrong based on everything that has come out. And I'm like, how are y'all willing to put this on your website? Yeah, yeah. So I do see that. But I also understand that there is a want and a need for those curious to seek knowledge. To have better, more robust, more concise, and more efficient workflows. So that they actually can search for knowledge faster. I mean, that ultimately is what all of this is about because I cannot tell you how many friends I've had who've wasted the first year and a half of their PhD trying to just reproduce someone else's study, not even replicated. Just get the code and the data to work out. Absolutely. And it ultimately ends up failing. And it's terrible as a scientist is an early career like PhD because you want to spend that time creating breakthroughs. Of course. And these early stage years in your career are so incredibly formative. The fact that you just wasted a year and a half of your life trying to reproduce someone else's work. It's such a waste. You want to analyze the data. And exactly. There's some horrible figures out there. I saw figures where almost 80% of all the, let's say all the work that the researchers do is around data munging. But it's just, I mean, it's playing with the data trying to get the data right, trying to, you know, interpret the data trying not analyzing it. You know, that that's only a small part of their work, which is a pity, which is a waste. Because you want them to, to analyze data and come with strong conclusions breakthroughs, yeah, and not spend, you know, three quarter of the day munging that that is such a waste of time. Ultimately, I think what everyone here at this foundation is trying to do is make it as easy as humanly possible for smart people to answer hard questions. I think that's, that's what all of this comes down to. And it is something that takes everyone, but humanity as a species is going to be facing increasingly more difficult challenges as we move forward. Climate change is not going to get better. A lot of the issues around disease that are starting to come up are only going to get worse and worse and worse. And if we continue to move forward with an engine of human progress, which is what I would call science and this concept science and technology together, with one that, you know, 80% of your time is spent fixing flaws that are inherent in the system. We're not going to be able to solve a lot of these. And ultimately, any government's answer to like global warming is going to be throwing money at academia, trying to get smart people to come up with the solution. We want to make sure that we can make the most out of that as possible and give ourselves the best chance of fighting some of these problems. That's ultimately what this is about my mind. And I'll say that that is, it's a fun thing to work towards. It's also used to be part of this. I feel that also. And I'm 100% convinced that when we talk again in 10, 15 years time, we will look back and say, do you recall these days that I was enlightened? And at that time, it will be fully up and running and we will have, we will be further down this line. So many steps. But it will be a great journey to be part of it because this will happen. This will happen. It has to. It has to. And the only thing we don't know is how fast will it go? Well, to me, one of the biggest things that we have to be aware of and that we have to be wary of is how complex some of this gets. If someone is a scientist who, let's say they focus on biology, right? They don't necessarily know what non-thology is and it shouldn't be their main focus. They should focus on biology. It's as you get further and further into both the red and the blue principles, things can get tricky. It can get tricky. Ultimately, it is about one of our challenges as a community is making it as easy as humanly possible for scientists to trust the work of other scientists. And one of the things that I actually would love to focus on more as a community is the importance of UI UX. How do we use technology? How do we use application interfaces and some of the amazing work that's been done around design and psychology to make it easier to make it maybe at some point, hopefully fingers crossed for people to go through and think about the questions of the metadata. It's a very, very tough challenge, but I was reading a paper the other day, which quoted Tim Berners-Lee basically saying the internet should be all around linked data. We shouldn't have any of these fancy interface bottles and ridiculousness. And I'm like, well, yes, I agree, but there always has to be a human component. You just need that. You do. You need a clean, simple user interface. And that's, I think, at least for my time at helping the foundation working with the foundation is one of the things that I'd like to move towards is figuring out how we get professional developers here, how we bring in the cavalry of people who have been coding front-ends since they were 10 years old. Because this is the kind of vision and mission that a computer scientist, a professional developer understands. Like building machine action ability, yeah, that's awesome. But there's something to be said for developer experience. And that's one of the points within fair that I think I may have to advocate for as we move forward is, you know, what does it look like for a developer to come into fair? And the same way that a biologist has to learn all of these complex concepts, does the developer have to learn come in and learn all of these complex concepts? Like how do we make it as simple for them to come in, not make impact and make money as humanly possible? That's to me that is going to be absolutely critical. Yeah, I can see that also. That is critical. And of course, I think we have a good starting point at all these data stewards. Yes. You see now the majority of the Dutch universities hiring data stewards. Really? Wow. And in the old days, they would maybe hire one and then try to have this person trying for them to support maybe 20, 25 scientists. And now we see the ratio is actually changing. So we see more and more data stewards being hired so that they can maybe help, let's say, five to 10 scientists with the data. Because I fully understand that, you know, a scientist wants to concentrate on the domain, on the subjects. Yeah. He wants to think about, you know, the intellectual nuts to be cracked in his area of expertise. Not necessarily, he wants to spend a lot of time on data. So if he is supported by data steward, really understands all the things that we've been talking about now in the last 10 minutes, that would be fantastic. And of course, if you think about the data, you can make a move. If you then create a network of data stewards who exchange data, who exchange ideas, who exchange software, who understand the process and, you know, all the different steps that need to be done. And you know, you can, you can make some huge steps in this area. I completely agree. So what, what do you see the responsibility kind of that the split in responsibilities between the scientist and the data steward with regards to making data fair? Well, I would think that the scientists has to understand what needs to be done at a maybe at a high level at least. But I would say the real responsibility of the data and the metadata should really be in the hands of the data steward. So it's, I would say it's an enormous advantage if a data steward also understands the domain. Because then he can take some intelligence recommendations and discussions that he would have with the data scientist. If he is completely unaware of the field that the scientists is working in, it is probably difficult. So a data steward who understands astronomy will probably be a much better data steward for the scientist as compared to a person who hasn't got a clue what this scientist is actually doing. So that's why it's not, I don't see it as a very simple job, you know, which you can just hand out to somebody who's trained, let's say two or three weeks and then oh, you're now a data steward. No, really has to person has to be a person with maybe not the deepest but at least some insight in terms of what is happening from a scientific point of view. And it's not, but I think as you go through training as a data steward, right, you're going to specialize in specific areas, right. Let's say you were a data steward with the focus on privacy, with the focus on big data, the focus on enabling data visiting, right. Whenever I think of data steward ship, honestly, I think of it in terms of college majors and minors major to your major is data stewardship and your minor is big day or your minor is, you know, any of these different concepts and what you eventually see what I would hope we would see is some of these different minors being. I guess put together to fit the needs of individual fields so that once as a data steward, let's say coming straight out of school, you got your data stewardship major, you got a minor in big data and privacy and then you went on to genetics, right. Perfect. Awesome. You may not know anything about genetics at the start, but you understand enough about the base needs of the data that you can pick it up. Hopefully you have a certain level of intellectual curiosity where you want to learn. And hopefully the people who you're working with have the same intellectual curiosity where they want to become an IT professional of sorts. And that interface and that linkage, once again, we have to make this something where building a personal relationship with the data steward gets a scientist tenure. That's how you get your citation is by well formatted data and machines accessing that constantly. It's the reward system. It's the incentive that comes into it and it's building a person to person relationship with a high quality individual managing your data. But that means that you also in the intensive systems, you have to build that in. You have basically to give the scientists also credit. We need to move past the H&X quite frankly. Absolutely. Because that is holding us back. I would say it's a couple different things. I would also go so far as to say that the DOI is not a persistent identifier that can handle the concept of agile science that I listed earlier. There are a couple different legacy pieces of architecture or legacy concepts, I suppose, that we're not made for the digital world. Okay, they've done an amazing job getting us to the point where we're at. It's not a question of in my mind, trashing the H&X although I guess I did just kind of do that. It's appreciating the H&X for the brilliant system that it was whenever citations happen through mail. And understanding that this really is building on the shoulders of giants and that it is another iteration. Can I stop you here for a minute? Sure. I need to have a meeting.