

In this conversation, Eric and Bert discuss how to avoid vendor lock-in for data storage.
Bert is saying that when data is exchanged between multiple parties, it's important to avoid vendor lock-in, which can happen when one organization takes on the responsibility of managing all the data.


The presentation will explore the idea of fair, which is an acronym for "findable, accessible, interoperable, and reusable." The idea is that data should be easy to find, access, use, and share. One of the goals of fair is to promote data exchange based on a federated decentralized structure, which means that data should be easy to find and use regardless of where it is stored or how it is formatted.


It is important to have clear metadata when exchanging data between parties, in order to avoid vendor lock-in and the creation of monopolies.


There are two main types of vendor lock-in: data lock-in through a lack of interoperability, or proprietary metadata formats; and data lock-in through silos.


It's hard to say which one is more problematic or has a stronger lock-in without more context.
There are many silos in the modern world, both in industry and elsewhere. These silos make it difficult for organizations to communicate and exchange data with each other.
In the industry, silos have been created by systems that were built in the past. These systems used different standards, vocabularies, and terms which made it difficult to bring these worlds together.
There is a certain attitude to impose systems and standards upon others, when in reality, the fair principles say to keep your standards and use what you want. The fair principles bridge the gap between different systems.


In Dutch, we talk about having one common language for care, medical care, and other service areas. However, the fair principles disrupt this by saying that you don't need a single language for everybody to use. Instead, you can use your own standards and still be able to communicate effectively. The challenge is to open up silos by enriching data with enough metadata so that machines can understand what is meant.


The fair principles were designed for machines, not humans. The idea is that data should be enriched with metadata so that machines can interpret the data correctly. This metadata should include all the relevant information about a dataset.


The work that fair does can be seen as a translation layer between different people and machines. It allows for communication and collaboration in a more efficient way.

Fair is best described as Google Translate for machines. It allows for ontologies to be created which provides a way for different machines to communicate with each other by translating between different languages. This is important on both an industry-wide level, where different companies use different terminology, and on a subfield level, where different scientists might use different terminology for the same thing.
Came up with this specific gene first. And if that name is the one that continues on, then my paper is the one that gets cited.  And to a certain extent, it's the same thing with standards that we talked about above.


This paragraph discusses the idea that machines need to be able to interpret data in order to make it useful for humans. The point is made that humans can often understand data that is meant for machines, but the reverse is not always true.
In order for data exchange to be clear for both machines and humans, it is important that the context surrounding the data is taken into account. Not all data needs to be enriched with the same type of metadata.


The metadata associated with research data is crucial in terms of the research itself, but also in terms of all the circumstances under which the research was conducted. This makes the research stronger because then another person can replicate the experiment that was done.
The H-index is a measure of the impact of a scientist's work. And while it can be useful, it also has its flaws. Scientists are very clever and have figured out how to manipulate the H index as they should. That's how you move your career forward.


A citation can be thought of as someone reusing an idea in one of your papers. A machine going through accessing, querying, and computing on your data can be thought of as a machine reusing one of your ideas. It's important to note that just because data is fair doesn't mean that it's 100% accurate - there may be errors in the data itself or in the way it was collected. Therefore, it's important to include metadata about the provenance of the data to ensure its accuracy.


It is important not only for the machines, but also for the humans involved in data management to be aware of and understand the implications of the FAIR principles. This includes things like understanding where the data is coming from and whether or not it is trustworthy.


This data is coming from a variety of sources, but the idea of creation, de-curation pipelines is something that I've been enthralled with for quite some time now. The idea that from the very moment that let's say a mass spec machine is taking a measurement, there is a cryptographic stamp that issues provenance. And then as it goes into its next phase of formatting, the compute that is run on that data, just to reformat it, has a stamp.
The difference between a mass spec file and a formatted file is that the latter goes through multiple steps of analysis before being published. Once it goes through these steps and is eventually curated by humans, the complete flow of fare is data provenance - which is the history of where data comes from - and compute provenance, which is the history of where computations come from.
It is crucial to have a complete trail of everything that has been done in order to increase the trust level of data by magnitudes.
The mistrust or distrust of data sharing is what has held back the fair data concept for so long. But when you look at the fair principles and how they create a chain of trust, it's a step in the right direction. And when you add blockchain into the mix, it's another huge step forward in terms of security and safety.


In the world of computer science, 20-30 years ago, the problem of collaboration in software development was faced. The early days of coding entailed saving a file and emailing it to a coworker with the hope of getting something in return. If you received a random file attachment via email, there was no way to understand its provenance or what it was. In today's age, no one would open such a file under any circumstance.


In the late 1990s, the Agile manifesto came out. This was a set of principles for software development that emphasize things like speed, flexibility, and customer collaboration. The Agile manifesto has been very influential in the software development world and has shaped how many developers work today.
The idea of small, continuous steps to show your work and create a chain of trust started to become more realistic with software development in 2001 when the Agile Manifesto was codified. This took off even more in 2005 when Linus Torvalds created Git, and 2008 when GitHub was released. If you look at any GitHub repository, you can see the work that has been put into it and understand if it is a reliable source.

So, like I can look into bits of the code and say, okay, yeah, this does not seem to be formatted correctly. I'm not going to use this. Whereas I go to another repository, it's got 75 stars. It's got 25 different contributors.
There are a variety of metrics around trust, transparency, collaboration, and good lord- a lot of buzzwords. I apologize.


There is a lot of overlap between the agile manifesto and scientific research, particularly when it comes to provenance (tracking the origins of data and results). In software development, it's not uncommon to have multiple functions that accomplish the same goal, but in science, this can be a problem.
It's critical that we establish provenance and work to bust data silos, but it's also a challenging question because anyone who holds the record of provenance becomes the single source of truth.


If we want to have provenance - a reliable record of the history of something - we need to be able to trust the platform that this record is kept on. And this is where blockchain comes in. Blockchain provides a tamper-proof platform that can be used to store data, ensuring that it is not altered or deleted.


The term "trustless system" is used to describe a system where there is no need for trust between parties. Scientific data is one area where trustless systems are particularly important, as the data needs to be accurate and unbiased. However, implementing trustless systems can be complex, and there is a lot of "pseudo-fair" out there, where people claim to be doing fair things but aren't actually following the principles.
Some organizations think that having a website and a DOI is all that is needed to be considered fair, but this is not the case. There are many other factors that go into being considered fair and organizations need to be careful to make sure they are meeting all the criteria.


I've been involved in a project in the Nordic countries that was financed by the European Open Science Cloud. And it was exactly what we were doing there. There were a large number of repositories who showed their data, came with their data. We had some evaluated software running where we could actually guide them into the right directions and saying, well, you know, here are a few elements that you can actually add to your data to improve that level of fairness. And you know, it took us, what is it.
It is a challenge to get repositories to the level where they are compliant with the fair principles, but it is worth the effort.
The challenge now is to train more people in this area so that organizations can make the right steps forward. The way that fair has caught on as a grassroots movement in academia is impressive to me.
It's been very impressive to watch the fair principles mature and gain traction in the data community. The hardest part is getting buy-in from the community, but it is essential to have that grassroots ethos in order to make progress.
The author is discussing the fair principles and how they are often misinterpreted or misunderstood. They state that there is a want and a need for those curious to seek knowledge, and that this is what all of this (the fair principles) is about.
The fact that you just wasted a year and a half of your life trying to reproduce someone else's work is such a waste. You want to analyze the data.


A huge amount of the work that researchers do is around data munging - trying to get the data right, interpret it, and analyze it. This can be a very time-consuming process.
The goal of the GoFair foundation is to make it easier for people to answer hard questions by reducing the time they spend on data munging. This will allow humanity to better face difficult challenges such as climate change.
The goal of the GoFair Foundation is to make the most out of the money that governments are throwing at academia to try and find solutions to global problems. This includes issues such as disease and global warming. It is an important thing to work towards because it gives us the best chance of fighting these problems.
In 10-15 years, we will look back and say, “do you recall these days that I was enlightened?” And at that time, it will be fully up and running and we will have, we will be further down this line. So many steps But it will be a great journey to be part of it because this will happen.

The main point that George is trying to make is that the internet is constantly evolving and changing, and as it does, it gets more and more complex. Scientists should focus on their area of expertise and not worry about the complexities of the internet.
It can be tricky to adhere to both the red and blue principles because it ultimately comes down to scientists trusting the work of other scientists. One way we can make it easier for scientists is by focusing on UI/UX design, which takes into account the human component.
It is critical that we make it as easy as possible for developers to come in and make an impact with Fair. We need to make sure that the developer experience is clean and simple, so that they can easily understand the complex concepts involved. Otherwise, we run the risk of losing them entirely.


In the past, data stewards were hired by Dutch universities to support a small number of scientists. However, now many universities are hiring data stewards, showing the importance of this role in managing data.
The ratio is changing so that we see more data stewards being hired to help manage the data for 5-10 scientists. Scientists want to be able to focus on their domain and subjects, rather than worrying about data management.
George Straun does not want to spend a lot of time on data, but if he is supported by a data steward who understands all the things that have been talked about in the last 10 minutes, it would be fantastic. A network of data stewards who exchange data, ideas, software, and understand the process is necessary to create a global internet.

The responsibility of making data fair is split between the scientist and the data steward. The scientist needs to understand what needs to be done, but the real responsibility for the data and metadata should be with the data steward. It is an advantage if the data steward understands the domain, so that they can take intelligence recommendations and discussions with the data scientist.
It is difficult for a data steward to be effective if they are unaware of the field that the scientists is working in. Therefore, it is important that data stewards have a good understanding of the field before they are able to effectively do their job.


As a data steward, you will need to have some understanding of what is happening from a scientific point of view. You will specialize in specific areas, such as privacy, big data, or enabling data visiting.
In order to make sure data is high quality, we need to build personal relationships with data stewards. This means that we need to give scientists credit for their work in managing data. Additionally, we need to move past the H&X system and towards a more agile science that can handle the DOI as a persistent identifier.
There are a couple different legacy pieces of architecture or legacy concepts, I suppose, that we're not made for the digital world. Okay, they've done an amazing job getting us to the point where we're at.  It's not a question of in my mind, trashing the H&X although I guess I did just kind of do that.  It's appreciating the H&X for the brilliant system that it was whenever citations happen through mail.  And understanding that this really is building on the shoulders of giants and that it is another iteration.
