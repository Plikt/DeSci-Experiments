

In this conversation, Eric and Bert discuss how to avoid vendor lock-in for data storage.


It is important to have clear metadata when exchanging data between parties, in order to avoid vendor lock-in and the creation of monopolies.
There are two types of vendor lock-in: data lock-in through a lack of interoperability or proprietary metadata formats, and data lock-in through silos.


It's hard to say which one is more problematic or has a stronger lock-in without more context, but silos have been created by systems that were built in the past. These systems used different standards, vocabularies, and terms which made it difficult to bring these worlds together.
The fair principles were designed for machines, not humans. The idea is that data should be enriched with metadata so that machines can interpret the data correctly. This metadata should include all the relevant information about a dataset.
Fair is a translation layer between different people and machines that allows for communication and collaboration in a more efficient way. It is best described as Google Translate for machines, translating between different languages on both an industry-wide level with different companies using different terminology, and on a subfield level where different scientists might use different terminology for the same thing.


It is important that data exchange is clear for both machines and humans. In order for this to happen, the context surrounding the data must be taken into account. Not all data needs to be enriched with the same type of metadata.


The metadata associated with research data is crucial for replicating the experiment and for understanding all the circumstances under which the research was conducted.
It is important for both the machines and the humans involved in data management to be aware of and understand the implications of the FAIR principles. This includes understanding where the data is coming from and whether or not it is trustworthy.
In order to increase the trust level of data, it is crucial to have a complete trail of everything that has been done to it from the very beginning. This data should have cryptographic stamps at every stage of its journey from the mass spec machine to the final formatted product.
The mistrust of data sharing is what has held back the fair data concept for so long. But when you look at the fair principles and how they create a chain of trust, it's a step in the right direction. And when you add blockchain into the mix, it's another huge step forward in terms of security and safety.
The idea of continual, small steps to show your work and create a chain of trust started with software development in 2001 when the Agile Manifesto was codified. This became more prevalent in 2005 when Linus Torvalds created Git, and in 2008 when GitHub was released. If you look at any GitHub repository, you can see the work that has been put into it and understand if it is a reliable source.
There are a variety of metrics around trust, transparency, collaboration, and good lord- a lot of buzzwords. I apologize.

It's critical that we establish provenance and work to bust data silos, but it's also a challenging question because anyone who holds the record of provenance becomes the single source of truth.
In order to have provenance - a reliable record of the history of something - we need to be able to trust the platform that this record is kept on. Blockchain provides a tamper-proof platform that can be used to store data, ensuring that it is not altered or deleted.
There is a lot of "pseudo-fair" out there, where people claim to be doing fair things but aren't actually following the principles. I was involved in a project in the Nordic countries that was financed by the European Open Science Cloud where we tried to help repositories improve the fairness of their data.


It is a challenge to get repositories to the level where they are compliant with the fair principles, but it is worth the effort. The hardest part is getting buy-in from the community, but it is essential to have that grassroots ethos in order to make progress.


The GoFair Foundation is all about data munging - trying to get the data right, interpret it, and analyze it. This is important work because it helps governments find solutions to global problems like disease and global warming.
George Straun believes that the internet is constantly evolving and changing, and as it does, it gets more and more complex. Scientists should focus on their area of expertise and not worry about the complexities of the internet. It is critical that we make it as easy as possible for developers to come in and make an impact with Fair.
In the past, data stewards were hired by Dutch universities to support a small number of scientists. However, now many universities are hiring data stewards, showing the importance of this role in managing data. George Straun does not want to spend a lot of time on data, but if he is supported by a data steward who understands all the things that have been talked about in the last 10 minutes, it would be fantastic. A network of data stewards who exchange data, ideas, software, and understand the process is necessary to create a global internet.


It is difficult for a data steward to be effective if they are unaware of the field that the scientists is working in. Therefore, it is important that data stewards have a good understanding of the field before they are able to effectively do their job.
As a data steward, you will need to have some understanding of what is happening from a scientific point of view. You will specialize in specific areas, such as privacy, big data, or enabling data visiting.
There are a couple different legacy pieces of architecture or legacy concepts, I suppose, that we're not made for the digital world. okay, they've done an amazing job getting us to the point where we're at. It's not a question of in my mind, trashing the H&X although I guess I did just kind of do that. It's appreciating the H&X for the brilliant system that it was whenever citations happen through mail. And understanding that this really is building on the shoulders of giants and that it is another iteration.
